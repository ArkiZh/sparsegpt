(torch2) [zk@localhost sparsegpt]$ /home/zk/miniconda3/envs/torch2/bin/python /home/zk/repos/sparsegpt/llama.py
[2023-08-18 04:25:01,301] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 33/33 [00:13<00:00,  2.53it/s]
Eval before sparse:
The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. 
The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. 
The class this function is called from is 'LlamaTokenizer'.
You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565
Dataset: wikitext2
Evaluating ...
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
Perplexity: 5.677063
The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. 
The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. 
The class this function is called from is 'LlamaTokenizer'.
Dataset: ptb
Evaluating ...
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
Perplexity: 10.107042
The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. 
The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. 
The class this function is called from is 'LlamaTokenizer'.
Repo card metadata block was not found. Setting CardData to empty.
Repo card metadata block was not found. Setting CardData to empty.
Dataset: c4
Evaluating ...
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
Perplexity: 7.343807
The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. 
The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. 
The class this function is called from is 'LlamaTokenizer'.
Repo card metadata block was not found. Setting CardData to empty.
Repo card metadata block was not found. Setting CardData to empty.
## Before sparse
====================================================================================================
model.embed_tokens.weight-------------------------------------------torch.nn.modules.sparse.Embedding--------------------------131072000----------[32000, 4096]---torch.float16  
model.layers.0.self_attn.q_proj.weight------------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.0.self_attn.k_proj.weight------------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.0.self_attn.v_proj.weight------------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.0.self_attn.o_proj.weight------------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.0.mlp.gate_proj.weight---------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.0.mlp.up_proj.weight-----------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.0.mlp.down_proj.weight---------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[4096, 11008]---torch.float16  
model.layers.0.input_layernorm.weight-------------------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.0.post_attention_layernorm.weight----------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.1.self_attn.q_proj.weight------------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.1.self_attn.k_proj.weight------------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.1.self_attn.v_proj.weight------------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.1.self_attn.o_proj.weight------------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.1.mlp.gate_proj.weight---------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.1.mlp.up_proj.weight-----------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.1.mlp.down_proj.weight---------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[4096, 11008]---torch.float16  
model.layers.1.input_layernorm.weight-------------------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.1.post_attention_layernorm.weight----------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.2.self_attn.q_proj.weight------------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.2.self_attn.k_proj.weight------------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.2.self_attn.v_proj.weight------------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.2.self_attn.o_proj.weight------------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.2.mlp.gate_proj.weight---------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.2.mlp.up_proj.weight-----------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.2.mlp.down_proj.weight---------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[4096, 11008]---torch.float16  
model.layers.2.input_layernorm.weight-------------------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.2.post_attention_layernorm.weight----------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.3.self_attn.q_proj.weight------------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.3.self_attn.k_proj.weight------------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.3.self_attn.v_proj.weight------------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.3.self_attn.o_proj.weight------------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.3.mlp.gate_proj.weight---------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.3.mlp.up_proj.weight-----------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.3.mlp.down_proj.weight---------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[4096, 11008]---torch.float16  
model.layers.3.input_layernorm.weight-------------------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.3.post_attention_layernorm.weight----------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.4.self_attn.q_proj.weight------------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.4.self_attn.k_proj.weight------------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.4.self_attn.v_proj.weight------------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.4.self_attn.o_proj.weight------------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.4.mlp.gate_proj.weight---------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.4.mlp.up_proj.weight-----------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.4.mlp.down_proj.weight---------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[4096, 11008]---torch.float16  
model.layers.4.input_layernorm.weight-------------------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.4.post_attention_layernorm.weight----------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.5.self_attn.q_proj.weight------------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.5.self_attn.k_proj.weight------------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.5.self_attn.v_proj.weight------------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.5.self_attn.o_proj.weight------------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.5.mlp.gate_proj.weight---------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.5.mlp.up_proj.weight-----------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.5.mlp.down_proj.weight---------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[4096, 11008]---torch.float16  
model.layers.5.input_layernorm.weight-------------------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.5.post_attention_layernorm.weight----------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.6.self_attn.q_proj.weight------------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.6.self_attn.k_proj.weight------------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.6.self_attn.v_proj.weight------------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.6.self_attn.o_proj.weight------------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.6.mlp.gate_proj.weight---------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.6.mlp.up_proj.weight-----------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.6.mlp.down_proj.weight---------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[4096, 11008]---torch.float16  
model.layers.6.input_layernorm.weight-------------------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.6.post_attention_layernorm.weight----------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.7.self_attn.q_proj.weight------------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.7.self_attn.k_proj.weight------------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.7.self_attn.v_proj.weight------------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.7.self_attn.o_proj.weight------------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.7.mlp.gate_proj.weight---------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.7.mlp.up_proj.weight-----------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.7.mlp.down_proj.weight---------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[4096, 11008]---torch.float16  
model.layers.7.input_layernorm.weight-------------------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.7.post_attention_layernorm.weight----------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.8.self_attn.q_proj.weight------------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.8.self_attn.k_proj.weight------------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.8.self_attn.v_proj.weight------------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.8.self_attn.o_proj.weight------------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.8.mlp.gate_proj.weight---------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.8.mlp.up_proj.weight-----------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.8.mlp.down_proj.weight---------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[4096, 11008]---torch.float16  
model.layers.8.input_layernorm.weight-------------------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.8.post_attention_layernorm.weight----------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.9.self_attn.q_proj.weight------------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.9.self_attn.k_proj.weight------------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.9.self_attn.v_proj.weight------------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.9.self_attn.o_proj.weight------------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.9.mlp.gate_proj.weight---------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.9.mlp.up_proj.weight-----------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.9.mlp.down_proj.weight---------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[4096, 11008]---torch.float16  
model.layers.9.input_layernorm.weight-------------------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.9.post_attention_layernorm.weight----------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.10.self_attn.q_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.10.self_attn.k_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.10.self_attn.v_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.10.self_attn.o_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.10.mlp.gate_proj.weight--------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.10.mlp.up_proj.weight----------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.10.mlp.down_proj.weight--------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[4096, 11008]---torch.float16  
model.layers.10.input_layernorm.weight------------------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.10.post_attention_layernorm.weight---------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.11.self_attn.q_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.11.self_attn.k_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.11.self_attn.v_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.11.self_attn.o_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.11.mlp.gate_proj.weight--------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.11.mlp.up_proj.weight----------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.11.mlp.down_proj.weight--------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[4096, 11008]---torch.float16  
model.layers.11.input_layernorm.weight------------------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.11.post_attention_layernorm.weight---------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.12.self_attn.q_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.12.self_attn.k_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.12.self_attn.v_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.12.self_attn.o_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.12.mlp.gate_proj.weight--------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.12.mlp.up_proj.weight----------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.12.mlp.down_proj.weight--------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[4096, 11008]---torch.float16  
model.layers.12.input_layernorm.weight------------------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.12.post_attention_layernorm.weight---------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.13.self_attn.q_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.13.self_attn.k_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.13.self_attn.v_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.13.self_attn.o_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.13.mlp.gate_proj.weight--------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.13.mlp.up_proj.weight----------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.13.mlp.down_proj.weight--------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[4096, 11008]---torch.float16  
model.layers.13.input_layernorm.weight------------------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.13.post_attention_layernorm.weight---------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.14.self_attn.q_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.14.self_attn.k_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.14.self_attn.v_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.14.self_attn.o_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.14.mlp.gate_proj.weight--------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.14.mlp.up_proj.weight----------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.14.mlp.down_proj.weight--------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[4096, 11008]---torch.float16  
model.layers.14.input_layernorm.weight------------------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.14.post_attention_layernorm.weight---------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.15.self_attn.q_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.15.self_attn.k_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.15.self_attn.v_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.15.self_attn.o_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.15.mlp.gate_proj.weight--------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.15.mlp.up_proj.weight----------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.15.mlp.down_proj.weight--------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[4096, 11008]---torch.float16  
model.layers.15.input_layernorm.weight------------------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.15.post_attention_layernorm.weight---------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.16.self_attn.q_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.16.self_attn.k_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.16.self_attn.v_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.16.self_attn.o_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.16.mlp.gate_proj.weight--------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.16.mlp.up_proj.weight----------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.16.mlp.down_proj.weight--------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[4096, 11008]---torch.float16  
model.layers.16.input_layernorm.weight------------------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.16.post_attention_layernorm.weight---------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.17.self_attn.q_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.17.self_attn.k_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.17.self_attn.v_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.17.self_attn.o_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.17.mlp.gate_proj.weight--------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.17.mlp.up_proj.weight----------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.17.mlp.down_proj.weight--------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[4096, 11008]---torch.float16  
model.layers.17.input_layernorm.weight------------------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.17.post_attention_layernorm.weight---------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.18.self_attn.q_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.18.self_attn.k_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.18.self_attn.v_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.18.self_attn.o_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.18.mlp.gate_proj.weight--------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.18.mlp.up_proj.weight----------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.18.mlp.down_proj.weight--------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[4096, 11008]---torch.float16  
model.layers.18.input_layernorm.weight------------------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.18.post_attention_layernorm.weight---------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.19.self_attn.q_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.19.self_attn.k_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.19.self_attn.v_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.19.self_attn.o_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.19.mlp.gate_proj.weight--------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.19.mlp.up_proj.weight----------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.19.mlp.down_proj.weight--------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[4096, 11008]---torch.float16  
model.layers.19.input_layernorm.weight------------------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.19.post_attention_layernorm.weight---------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.20.self_attn.q_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.20.self_attn.k_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.20.self_attn.v_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.20.self_attn.o_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.20.mlp.gate_proj.weight--------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.20.mlp.up_proj.weight----------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.20.mlp.down_proj.weight--------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[4096, 11008]---torch.float16  
model.layers.20.input_layernorm.weight------------------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.20.post_attention_layernorm.weight---------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.21.self_attn.q_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.21.self_attn.k_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.21.self_attn.v_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.21.self_attn.o_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.21.mlp.gate_proj.weight--------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.21.mlp.up_proj.weight----------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.21.mlp.down_proj.weight--------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[4096, 11008]---torch.float16  
model.layers.21.input_layernorm.weight------------------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.21.post_attention_layernorm.weight---------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.22.self_attn.q_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.22.self_attn.k_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.22.self_attn.v_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.22.self_attn.o_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.22.mlp.gate_proj.weight--------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.22.mlp.up_proj.weight----------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.22.mlp.down_proj.weight--------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[4096, 11008]---torch.float16  
model.layers.22.input_layernorm.weight------------------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.22.post_attention_layernorm.weight---------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.23.self_attn.q_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.23.self_attn.k_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.23.self_attn.v_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.23.self_attn.o_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.23.mlp.gate_proj.weight--------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.23.mlp.up_proj.weight----------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.23.mlp.down_proj.weight--------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[4096, 11008]---torch.float16  
model.layers.23.input_layernorm.weight------------------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.23.post_attention_layernorm.weight---------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.24.self_attn.q_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.24.self_attn.k_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.24.self_attn.v_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.24.self_attn.o_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.24.mlp.gate_proj.weight--------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.24.mlp.up_proj.weight----------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.24.mlp.down_proj.weight--------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[4096, 11008]---torch.float16  
model.layers.24.input_layernorm.weight------------------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.24.post_attention_layernorm.weight---------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.25.self_attn.q_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.25.self_attn.k_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.25.self_attn.v_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.25.self_attn.o_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.25.mlp.gate_proj.weight--------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.25.mlp.up_proj.weight----------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.25.mlp.down_proj.weight--------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[4096, 11008]---torch.float16  
model.layers.25.input_layernorm.weight------------------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.25.post_attention_layernorm.weight---------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.26.self_attn.q_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.26.self_attn.k_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.26.self_attn.v_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.26.self_attn.o_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.26.mlp.gate_proj.weight--------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.26.mlp.up_proj.weight----------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.26.mlp.down_proj.weight--------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[4096, 11008]---torch.float16  
model.layers.26.input_layernorm.weight------------------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.26.post_attention_layernorm.weight---------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.27.self_attn.q_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.27.self_attn.k_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.27.self_attn.v_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.27.self_attn.o_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.27.mlp.gate_proj.weight--------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.27.mlp.up_proj.weight----------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.27.mlp.down_proj.weight--------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[4096, 11008]---torch.float16  
model.layers.27.input_layernorm.weight------------------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.27.post_attention_layernorm.weight---------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.28.self_attn.q_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.28.self_attn.k_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.28.self_attn.v_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.28.self_attn.o_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.28.mlp.gate_proj.weight--------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.28.mlp.up_proj.weight----------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.28.mlp.down_proj.weight--------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[4096, 11008]---torch.float16  
model.layers.28.input_layernorm.weight------------------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.28.post_attention_layernorm.weight---------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.29.self_attn.q_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.29.self_attn.k_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.29.self_attn.v_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.29.self_attn.o_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.29.mlp.gate_proj.weight--------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.29.mlp.up_proj.weight----------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.29.mlp.down_proj.weight--------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[4096, 11008]---torch.float16  
model.layers.29.input_layernorm.weight------------------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.29.post_attention_layernorm.weight---------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.30.self_attn.q_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.30.self_attn.k_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.30.self_attn.v_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.30.self_attn.o_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.30.mlp.gate_proj.weight--------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.30.mlp.up_proj.weight----------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.30.mlp.down_proj.weight--------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[4096, 11008]---torch.float16  
model.layers.30.input_layernorm.weight------------------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.30.post_attention_layernorm.weight---------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.31.self_attn.q_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.31.self_attn.k_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.31.self_attn.v_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.31.self_attn.o_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.31.mlp.gate_proj.weight--------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.31.mlp.up_proj.weight----------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.31.mlp.down_proj.weight--------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[4096, 11008]---torch.float16  
model.layers.31.input_layernorm.weight------------------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.31.post_attention_layernorm.weight---------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.norm.weight---------------------------------------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
lm_head.weight------------------------------------------------------torch.nn.modules.linear.Linear-----------------------------131072000----------[32000, 4096]---torch.float16  
Total num: 6738415616 is 6.738415616 Billion. GPU mem: 12.582534790039062 GB
End. Before sparse 


Starting...
Ready.
0 self_attn.q_proj
Pruning ...
time 1.95
error 295.60711669921875
0 self_attn.k_proj
Pruning ...
time 0.79
error 371.8988037109375
0 self_attn.v_proj
Pruning ...
time 0.71
error 94.84304809570312
0 self_attn.o_proj
Pruning ...
time 0.69
error 10.224353790283203
0 mlp.gate_proj
Pruning ...
time 0.69
error 1505.052001953125
0 mlp.up_proj
Pruning ...
time 0.79
error 1425.9505615234375
0 mlp.down_proj
Pruning ...
time 2.19
error 35.31840133666992
1 self_attn.q_proj
Pruning ...
time 1.51
error 3060.439453125
1 self_attn.k_proj
Pruning ...
time 0.73
error 3076.384765625
1 self_attn.v_proj
Pruning ...
time 0.77
error 498.6512451171875
1 self_attn.o_proj
Pruning ...
time 0.69
error 74.51463317871094
1 mlp.gate_proj
Pruning ...
time 0.67
error 6990.92236328125
1 mlp.up_proj
Pruning ...
time 0.68
error 6291.45751953125
1 mlp.down_proj
Pruning ...
time 2.03
error 239.72604370117188
2 self_attn.q_proj
Pruning ...
time 1.49
error 11413.39453125
2 self_attn.k_proj
Pruning ...
time 0.67
error 10734.6318359375
2 self_attn.v_proj
Pruning ...
time 0.68
error 2121.13330078125
2 self_attn.o_proj
Pruning ...
time 0.67
error 195.00567626953125
2 mlp.gate_proj
Pruning ...
time 0.68
error 17695.984375
2 mlp.up_proj
Pruning ...
time 0.80
error 14677.314453125
2 mlp.down_proj
Pruning ...
time 2.37
error 2990.54248046875
3 self_attn.q_proj
Pruning ...
time 1.63
error 28573.896484375
3 self_attn.k_proj
Pruning ...
time 0.91
error 28704.025390625
3 self_attn.v_proj
Pruning ...
time 0.83
error 10607.25
3 self_attn.o_proj
Pruning ...
time 0.91
error 208.28500366210938
3 mlp.gate_proj
Pruning ...
time 0.82
error 30055.716796875
3 mlp.up_proj
Pruning ...
time 0.83
error 25428.8125
3 mlp.down_proj
Pruning ...
time 2.57
error 1080.514892578125
4 self_attn.q_proj
Pruning ...
time 1.63
error 45053.265625
4 self_attn.k_proj
Pruning ...
time 0.82
error 43876.65625
4 self_attn.v_proj
Pruning ...
time 0.95
error 15466.171875
4 self_attn.o_proj
Pruning ...
time 0.81
error 449.52008056640625
4 mlp.gate_proj
Pruning ...
time 0.72
error 43020.3671875
4 mlp.up_proj
Pruning ...
time 0.86
error 36077.42578125
4 mlp.down_proj
Pruning ...
time 2.49
error 1774.3504638671875
5 self_attn.q_proj
Pruning ...
time 1.62
error 68271.28125
5 self_attn.k_proj
Pruning ...
time 0.81
error 67705.96875
5 self_attn.v_proj
Pruning ...
time 0.83
error 27260.66015625
5 self_attn.o_proj
Pruning ...
time 0.81
error 626.092529296875
5 mlp.gate_proj
Pruning ...
time 0.85
error 59469.25
5 mlp.up_proj
Pruning ...
time 0.89
error 47901.21484375
5 mlp.down_proj
Pruning ...
time 2.45
error 2823.3349609375
6 self_attn.q_proj
Pruning ...
time 1.48
error 75300.1015625
6 self_attn.k_proj
Pruning ...
time 0.67
error 75795.875
6 self_attn.v_proj
Pruning ...
time 0.67
error 29028.41015625
6 self_attn.o_proj
Pruning ...
time 0.74
error 1083.117431640625
6 mlp.gate_proj
Pruning ...
time 0.67
error 65261.67578125
6 mlp.up_proj
Pruning ...
time 0.68
error 55412.1640625
6 mlp.down_proj
Pruning ...
time 2.11
error 3714.263916015625
7 self_attn.q_proj
Pruning ...
time 1.50
error 74731.4375
7 self_attn.k_proj
Pruning ...
time 0.75
error 74221.03125
7 self_attn.v_proj
Pruning ...
time 0.67
error 32576.80078125
7 self_attn.o_proj
Pruning ...
time 0.67
error 1786.667724609375
7 mlp.gate_proj
Pruning ...
time 0.76
error 69128.109375
7 mlp.up_proj
Pruning ...
time 0.67
error 60776.12109375
7 mlp.down_proj
Pruning ...
time 2.02
error 4711.80029296875
8 self_attn.q_proj
Pruning ...
time 1.49
error 81727.6953125
8 self_attn.k_proj
Pruning ...
time 0.67
error 80637.9765625
8 self_attn.v_proj
Pruning ...
time 0.77
error 37625.765625
8 self_attn.o_proj
Pruning ...
time 0.67
error 2684.78125
8 mlp.gate_proj
Pruning ...
time 0.67
error 72339.65625
8 mlp.up_proj
Pruning ...
time 0.67
error 64471.3828125
8 mlp.down_proj
Pruning ...
time 2.05
error 5599.70068359375
9 self_attn.q_proj
Pruning ...
time 1.49
error 84741.046875
9 self_attn.k_proj
Pruning ...
time 0.81
error 84186.109375
9 self_attn.v_proj
Pruning ...
time 0.80
error 42118.83203125
9 self_attn.o_proj
Pruning ...
time 0.78
error 3281.08251953125
9 mlp.gate_proj
Pruning ...
time 0.71
error 75957.9375
9 mlp.up_proj
Pruning ...
time 0.74
error 70783.0390625
9 mlp.down_proj
Pruning ...
time 2.01
error 6699.974609375
10 self_attn.q_proj
Pruning ...
time 1.55
error 96120.859375
10 self_attn.k_proj
Pruning ...
time 0.77
error 95274.5078125
10 self_attn.v_proj
Pruning ...
time 0.66
error 48653.2734375
10 self_attn.o_proj
Pruning ...
time 0.68
error 4687.24267578125
10 mlp.gate_proj
Pruning ...
time 0.75
error 82582.96875
10 mlp.up_proj
Pruning ...
time 0.67
error 79694.71875
10 mlp.down_proj
Pruning ...
time 2.02
error 8239.9072265625
11 self_attn.q_proj
Pruning ...
time 1.48
error 92604.234375
11 self_attn.k_proj
Pruning ...
time 0.66
error 92097.6640625
11 self_attn.v_proj
Pruning ...
time 0.66
error 45832.8203125
11 self_attn.o_proj
Pruning ...
time 0.66
error 6635.21875
11 mlp.gate_proj
Pruning ...
time 0.67
error 86102.078125
11 mlp.up_proj
Pruning ...
time 0.73
error 85537.9296875
11 mlp.down_proj
Pruning ...
time 2.08
error 10339.96484375
12 self_attn.q_proj
Pruning ...
time 1.52
error 107957.796875
12 self_attn.k_proj
Pruning ...
time 0.75
error 107863.96875
12 self_attn.v_proj
Pruning ...
time 0.68
error 58944.51171875
12 self_attn.o_proj
Pruning ...
time 0.67
error 7167.734375
12 mlp.gate_proj
Pruning ...
time 0.66
error 100135.171875
12 mlp.up_proj
Pruning ...
time 0.67
error 99218.21875
12 mlp.down_proj
Pruning ...
time 2.02
error 12765.515625
13 self_attn.q_proj
Pruning ...
time 1.48
error 116155.265625
13 self_attn.k_proj
Pruning ...
time 0.66
error 115756.296875
13 self_attn.v_proj
Pruning ...
time 0.67
error 71482.7578125
13 self_attn.o_proj
Pruning ...
time 0.68
error 6896.6943359375
13 mlp.gate_proj
Pruning ...
time 0.74
error 108838.34375
13 mlp.up_proj
Pruning ...
time 0.73
error 109963.046875
13 mlp.down_proj
Pruning ...
time 2.03
error 14838.609375
14 self_attn.q_proj
Pruning ...
time 1.49
error 116711.8515625
14 self_attn.k_proj
Pruning ...
time 0.68
error 116284.3125
14 self_attn.v_proj
Pruning ...
time 0.68
error 71495.9609375
14 self_attn.o_proj
Pruning ...
time 0.75
error 7845.4072265625
14 mlp.gate_proj
Pruning ...
time 0.74
error 120827.390625
14 mlp.up_proj
Pruning ...
time 0.68
error 123219.5625
14 mlp.down_proj
Pruning ...
time 2.05
error 17044.98828125
15 self_attn.q_proj
Pruning ...
time 1.53
error 116688.125
15 self_attn.k_proj
Pruning ...
time 0.67
error 115902.640625
15 self_attn.v_proj
Pruning ...
time 0.68
error 72277.953125
15 self_attn.o_proj
Pruning ...
time 0.77
error 10127.7734375
15 mlp.gate_proj
Pruning ...
time 0.69
error 131599.84375
15 mlp.up_proj
Pruning ...
time 0.68
error 133498.0625
15 mlp.down_proj
Pruning ...
time 2.12
error 21649.45703125
16 self_attn.q_proj
Pruning ...
time 1.62
error 120825.96875
16 self_attn.k_proj
Pruning ...
time 0.89
error 122634.859375
16 self_attn.v_proj
Pruning ...
time 0.79
error 81522.4765625
16 self_attn.o_proj
Pruning ...
time 0.80
error 9690.943359375
16 mlp.gate_proj
Pruning ...
time 0.83
error 150557.578125
16 mlp.up_proj
Pruning ...
time 0.87
error 149991.78125
16 mlp.down_proj
Pruning ...
time 2.44
error 28557.611328125
17 self_attn.q_proj
Pruning ...
time 1.70
error 126523.9296875
17 self_attn.k_proj
Pruning ...
time 0.88
error 127443.109375
17 self_attn.v_proj
Pruning ...
time 0.81
error 92369.5
17 self_attn.o_proj
Pruning ...
time 0.86
error 14156.89453125
17 mlp.gate_proj
Pruning ...
time 0.79
error 170154.1875
17 mlp.up_proj
Pruning ...
time 0.80
error 168516.53125
17 mlp.down_proj
Pruning ...
time 2.41
error 36141.09375
18 self_attn.q_proj
Pruning ...
time 1.60
error 123400.4921875
18 self_attn.k_proj
Pruning ...
time 0.80
error 122564.46875
18 self_attn.v_proj
Pruning ...
time 0.79
error 96734.484375
18 self_attn.o_proj
Pruning ...
time 0.74
error 15671.603515625
18 mlp.gate_proj
Pruning ...
time 0.87
error 194054.765625
18 mlp.up_proj
Pruning ...
time 0.82
error 187490.703125
18 mlp.down_proj
Pruning ...
time 2.38
error 42407.03125
19 self_attn.q_proj
Pruning ...
time 1.61
error 119616.6953125
19 self_attn.k_proj
Pruning ...
time 0.78
error 118645.953125
19 self_attn.v_proj
Pruning ...
time 0.78
error 102954.796875
19 self_attn.o_proj
Pruning ...
time 0.87
error 14664.97265625
19 mlp.gate_proj
Pruning ...
time 0.79
error 212929.75
19 mlp.up_proj
Pruning ...
time 0.78
error 201920.578125
19 mlp.down_proj
Pruning ...
time 2.40
error 49138.65625
20 self_attn.q_proj
Pruning ...
time 1.62
error 137334.75
20 self_attn.k_proj
Pruning ...
time 0.83
error 137732.96875
20 self_attn.v_proj
Pruning ...
time 0.89
error 118440.21875
20 self_attn.o_proj
Pruning ...
time 0.82
error 17410.78125
20 mlp.gate_proj
Pruning ...
time 0.80
error 230665.5
20 mlp.up_proj
Pruning ...
time 0.81
error 216559.390625
20 mlp.down_proj
Pruning ...
time 2.38
error 58570.78125
21 self_attn.q_proj
Pruning ...
time 1.62
error 126234.875
21 self_attn.k_proj
Pruning ...
time 0.86
error 124515.921875
21 self_attn.v_proj
Pruning ...
time 0.82
error 124102.8203125
21 self_attn.o_proj
Pruning ...
time 0.89
error 18566.39453125
21 mlp.gate_proj
Pruning ...
time 0.84
error 251417.9375
21 mlp.up_proj
Pruning ...
time 0.82
error 231565.21875
21 mlp.down_proj
Pruning ...
time 2.54
error 64913.140625
22 self_attn.q_proj
Pruning ...
time 1.68
error 143456.546875
22 self_attn.k_proj
Pruning ...
time 0.80
error 143437.453125
22 self_attn.v_proj
Pruning ...
time 0.82
error 129815.921875
22 self_attn.o_proj
Pruning ...
time 0.82
error 11674.1796875
22 mlp.gate_proj
Pruning ...
time 0.82
error 269005.15625
22 mlp.up_proj
Pruning ...
time 0.81
error 248417.8125
22 mlp.down_proj
Pruning ...
time 2.39
error 68850.21875
23 self_attn.q_proj
Pruning ...
time 1.53
error 129778.59375
23 self_attn.k_proj
Pruning ...
time 0.92
error 126968.9921875
23 self_attn.v_proj
Pruning ...
time 0.83
error 144385.6875
23 self_attn.o_proj
Pruning ...
time 0.84
error 21277.64453125
23 mlp.gate_proj
Pruning ...
time 0.80
error 284111.03125
23 mlp.up_proj
Pruning ...
time 0.82
error 262585.53125
23 mlp.down_proj
Pruning ...
time 2.54
error 75777.5
24 self_attn.q_proj
Pruning ...
time 1.67
error 143666.34375
24 self_attn.k_proj
Pruning ...
time 0.80
error 141938.125
24 self_attn.v_proj
Pruning ...
time 0.79
error 160028.6875
24 self_attn.o_proj
Pruning ...
time 0.79
error 18954.4609375
24 mlp.gate_proj
Pruning ...
time 0.78
error 299918.375
24 mlp.up_proj
Pruning ...
time 0.80
error 279779.75
24 mlp.down_proj
Pruning ...
time 2.37
error 78545.8359375
25 self_attn.q_proj
Pruning ...
time 1.61
error 168691.46875
25 self_attn.k_proj
Pruning ...
time 0.80
error 170890.453125
25 self_attn.v_proj
Pruning ...
time 0.80
error 166864.78125
25 self_attn.o_proj
Pruning ...
time 0.88
error 20529.234375
25 mlp.gate_proj
Pruning ...
time 0.81
error 313632.90625
25 mlp.up_proj
Pruning ...
time 0.93
error 294806.53125
25 mlp.down_proj
Pruning ...
time 2.34
error 84345.4296875
26 self_attn.q_proj
Pruning ...
time 1.59
error 180162.90625
26 self_attn.k_proj
Pruning ...
time 0.80
error 182397.921875
26 self_attn.v_proj
Pruning ...
time 0.78
error 194045.9375
26 self_attn.o_proj
Pruning ...
time 0.79
error 17730.025390625
26 mlp.gate_proj
Pruning ...
time 0.78
error 328379.1875
26 mlp.up_proj
Pruning ...
time 0.79
error 313932.65625
26 mlp.down_proj
Pruning ...
time 2.38
error 89753.1171875
27 self_attn.q_proj
Pruning ...
time 1.64
error 179996.59375
27 self_attn.k_proj
Pruning ...
time 0.79
error 182556.140625
27 self_attn.v_proj
Pruning ...
time 0.76
error 201652.25
27 self_attn.o_proj
Pruning ...
time 0.85
error 14053.7626953125
27 mlp.gate_proj
Pruning ...
time 0.82
error 339612.59375
27 mlp.up_proj
Pruning ...
time 0.79
error 329821.0625
27 mlp.down_proj
Pruning ...
time 2.41
error 94026.1328125
28 self_attn.q_proj
Pruning ...
time 1.66
error 155512.1875
28 self_attn.k_proj
Pruning ...
time 0.80
error 155312.625
28 self_attn.v_proj
Pruning ...
time 0.79
error 190987.375
28 self_attn.o_proj
Pruning ...
time 0.78
error 28085.50390625
28 mlp.gate_proj
Pruning ...
time 0.86
error 355400.5
28 mlp.up_proj
Pruning ...
time 0.85
error 352208.0625
28 mlp.down_proj
Pruning ...
time 2.35
error 107535.2265625
29 self_attn.q_proj
Pruning ...
time 1.64
error 165078.515625
29 self_attn.k_proj
Pruning ...
time 0.79
error 166873.84375
29 self_attn.v_proj
Pruning ...
time 0.79
error 213625.6875
29 self_attn.o_proj
Pruning ...
time 0.79
error 29272.283203125
29 mlp.gate_proj
Pruning ...
time 0.85
error 361354.75
29 mlp.up_proj
Pruning ...
time 0.85
error 363680.25
29 mlp.down_proj
Pruning ...
time 2.40
error 122978.859375
30 self_attn.q_proj
Pruning ...
time 1.66
error 156702.5
30 self_attn.k_proj
Pruning ...
time 0.79
error 158642.859375
30 self_attn.v_proj
Pruning ...
time 0.79
error 187297.734375
30 self_attn.o_proj
Pruning ...
time 0.78
error 29080.4453125
30 mlp.gate_proj
Pruning ...
time 0.83
error 352887.875
30 mlp.up_proj
Pruning ...
time 0.87
error 354892.25
30 mlp.down_proj
Pruning ...
time 2.43
error 145885.3125
31 self_attn.q_proj
Pruning ...
time 1.60
error 98352.65625
31 self_attn.k_proj
Pruning ...
time 0.80
error 101394.859375
31 self_attn.v_proj
Pruning ...
time 0.79
error 92351.078125
31 self_attn.o_proj
Pruning ...
time 0.77
error 23520.63671875
31 mlp.gate_proj
Pruning ...
time 0.79
error 282285.90625
31 mlp.up_proj
Pruning ...
time 0.76
error 277811.25
31 mlp.down_proj
Pruning ...
time 2.34
error 177615.890625
model.embed_tokens.weight tensor(1.4267e-06)
model.layers.0.self_attn.q_proj.weight tensor(0.5000)
model.layers.0.self_attn.k_proj.weight tensor(0.5000)
model.layers.0.self_attn.v_proj.weight tensor(0.5000)
model.layers.0.self_attn.o_proj.weight tensor(0.5000)
model.layers.0.mlp.gate_proj.weight tensor(0.5000)
model.layers.0.mlp.up_proj.weight tensor(0.5000)
model.layers.0.mlp.down_proj.weight tensor(0.5000)
530.9469363689423
## After sparse
====================================================================================================
model.embed_tokens.weight-------------------------------------------torch.nn.modules.sparse.Embedding--------------------------131072000----------[32000, 4096]---torch.float16  
model.layers.0.self_attn.q_proj.weight------------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.0.self_attn.k_proj.weight------------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.0.self_attn.v_proj.weight------------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.0.self_attn.o_proj.weight------------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.0.mlp.gate_proj.weight---------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.0.mlp.up_proj.weight-----------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.0.mlp.down_proj.weight---------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[4096, 11008]---torch.float16  
model.layers.0.input_layernorm.weight-------------------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.0.post_attention_layernorm.weight----------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.1.self_attn.q_proj.weight------------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.1.self_attn.k_proj.weight------------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.1.self_attn.v_proj.weight------------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.1.self_attn.o_proj.weight------------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.1.mlp.gate_proj.weight---------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.1.mlp.up_proj.weight-----------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.1.mlp.down_proj.weight---------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[4096, 11008]---torch.float16  
model.layers.1.input_layernorm.weight-------------------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.1.post_attention_layernorm.weight----------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.2.self_attn.q_proj.weight------------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.2.self_attn.k_proj.weight------------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.2.self_attn.v_proj.weight------------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.2.self_attn.o_proj.weight------------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.2.mlp.gate_proj.weight---------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.2.mlp.up_proj.weight-----------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.2.mlp.down_proj.weight---------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[4096, 11008]---torch.float16  
model.layers.2.input_layernorm.weight-------------------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.2.post_attention_layernorm.weight----------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.3.self_attn.q_proj.weight------------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.3.self_attn.k_proj.weight------------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.3.self_attn.v_proj.weight------------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.3.self_attn.o_proj.weight------------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.3.mlp.gate_proj.weight---------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.3.mlp.up_proj.weight-----------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.3.mlp.down_proj.weight---------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[4096, 11008]---torch.float16  
model.layers.3.input_layernorm.weight-------------------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.3.post_attention_layernorm.weight----------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.4.self_attn.q_proj.weight------------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.4.self_attn.k_proj.weight------------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.4.self_attn.v_proj.weight------------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.4.self_attn.o_proj.weight------------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.4.mlp.gate_proj.weight---------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.4.mlp.up_proj.weight-----------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.4.mlp.down_proj.weight---------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[4096, 11008]---torch.float16  
model.layers.4.input_layernorm.weight-------------------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.4.post_attention_layernorm.weight----------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.5.self_attn.q_proj.weight------------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.5.self_attn.k_proj.weight------------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.5.self_attn.v_proj.weight------------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.5.self_attn.o_proj.weight------------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.5.mlp.gate_proj.weight---------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.5.mlp.up_proj.weight-----------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.5.mlp.down_proj.weight---------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[4096, 11008]---torch.float16  
model.layers.5.input_layernorm.weight-------------------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.5.post_attention_layernorm.weight----------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.6.self_attn.q_proj.weight------------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.6.self_attn.k_proj.weight------------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.6.self_attn.v_proj.weight------------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.6.self_attn.o_proj.weight------------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.6.mlp.gate_proj.weight---------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.6.mlp.up_proj.weight-----------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.6.mlp.down_proj.weight---------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[4096, 11008]---torch.float16  
model.layers.6.input_layernorm.weight-------------------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.6.post_attention_layernorm.weight----------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.7.self_attn.q_proj.weight------------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.7.self_attn.k_proj.weight------------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.7.self_attn.v_proj.weight------------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.7.self_attn.o_proj.weight------------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.7.mlp.gate_proj.weight---------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.7.mlp.up_proj.weight-----------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.7.mlp.down_proj.weight---------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[4096, 11008]---torch.float16  
model.layers.7.input_layernorm.weight-------------------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.7.post_attention_layernorm.weight----------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.8.self_attn.q_proj.weight------------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.8.self_attn.k_proj.weight------------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.8.self_attn.v_proj.weight------------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.8.self_attn.o_proj.weight------------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.8.mlp.gate_proj.weight---------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.8.mlp.up_proj.weight-----------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.8.mlp.down_proj.weight---------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[4096, 11008]---torch.float16  
model.layers.8.input_layernorm.weight-------------------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.8.post_attention_layernorm.weight----------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.9.self_attn.q_proj.weight------------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.9.self_attn.k_proj.weight------------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.9.self_attn.v_proj.weight------------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.9.self_attn.o_proj.weight------------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.9.mlp.gate_proj.weight---------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.9.mlp.up_proj.weight-----------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.9.mlp.down_proj.weight---------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[4096, 11008]---torch.float16  
model.layers.9.input_layernorm.weight-------------------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.9.post_attention_layernorm.weight----------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.10.self_attn.q_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.10.self_attn.k_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.10.self_attn.v_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.10.self_attn.o_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.10.mlp.gate_proj.weight--------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.10.mlp.up_proj.weight----------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.10.mlp.down_proj.weight--------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[4096, 11008]---torch.float16  
model.layers.10.input_layernorm.weight------------------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.10.post_attention_layernorm.weight---------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.11.self_attn.q_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.11.self_attn.k_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.11.self_attn.v_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.11.self_attn.o_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.11.mlp.gate_proj.weight--------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.11.mlp.up_proj.weight----------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.11.mlp.down_proj.weight--------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[4096, 11008]---torch.float16  
model.layers.11.input_layernorm.weight------------------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.11.post_attention_layernorm.weight---------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.12.self_attn.q_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.12.self_attn.k_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.12.self_attn.v_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.12.self_attn.o_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.12.mlp.gate_proj.weight--------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.12.mlp.up_proj.weight----------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.12.mlp.down_proj.weight--------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[4096, 11008]---torch.float16  
model.layers.12.input_layernorm.weight------------------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.12.post_attention_layernorm.weight---------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.13.self_attn.q_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.13.self_attn.k_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.13.self_attn.v_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.13.self_attn.o_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.13.mlp.gate_proj.weight--------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.13.mlp.up_proj.weight----------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.13.mlp.down_proj.weight--------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[4096, 11008]---torch.float16  
model.layers.13.input_layernorm.weight------------------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.13.post_attention_layernorm.weight---------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.14.self_attn.q_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.14.self_attn.k_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.14.self_attn.v_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.14.self_attn.o_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.14.mlp.gate_proj.weight--------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.14.mlp.up_proj.weight----------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.14.mlp.down_proj.weight--------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[4096, 11008]---torch.float16  
model.layers.14.input_layernorm.weight------------------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.14.post_attention_layernorm.weight---------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.15.self_attn.q_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.15.self_attn.k_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.15.self_attn.v_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.15.self_attn.o_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.15.mlp.gate_proj.weight--------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.15.mlp.up_proj.weight----------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.15.mlp.down_proj.weight--------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[4096, 11008]---torch.float16  
model.layers.15.input_layernorm.weight------------------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.15.post_attention_layernorm.weight---------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.16.self_attn.q_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.16.self_attn.k_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.16.self_attn.v_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.16.self_attn.o_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.16.mlp.gate_proj.weight--------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.16.mlp.up_proj.weight----------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.16.mlp.down_proj.weight--------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[4096, 11008]---torch.float16  
model.layers.16.input_layernorm.weight------------------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.16.post_attention_layernorm.weight---------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.17.self_attn.q_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.17.self_attn.k_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.17.self_attn.v_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.17.self_attn.o_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.17.mlp.gate_proj.weight--------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.17.mlp.up_proj.weight----------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.17.mlp.down_proj.weight--------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[4096, 11008]---torch.float16  
model.layers.17.input_layernorm.weight------------------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.17.post_attention_layernorm.weight---------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.18.self_attn.q_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.18.self_attn.k_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.18.self_attn.v_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.18.self_attn.o_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.18.mlp.gate_proj.weight--------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.18.mlp.up_proj.weight----------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.18.mlp.down_proj.weight--------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[4096, 11008]---torch.float16  
model.layers.18.input_layernorm.weight------------------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.18.post_attention_layernorm.weight---------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.19.self_attn.q_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.19.self_attn.k_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.19.self_attn.v_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.19.self_attn.o_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.19.mlp.gate_proj.weight--------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.19.mlp.up_proj.weight----------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.19.mlp.down_proj.weight--------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[4096, 11008]---torch.float16  
model.layers.19.input_layernorm.weight------------------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.19.post_attention_layernorm.weight---------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.20.self_attn.q_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.20.self_attn.k_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.20.self_attn.v_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.20.self_attn.o_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.20.mlp.gate_proj.weight--------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.20.mlp.up_proj.weight----------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.20.mlp.down_proj.weight--------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[4096, 11008]---torch.float16  
model.layers.20.input_layernorm.weight------------------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.20.post_attention_layernorm.weight---------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.21.self_attn.q_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.21.self_attn.k_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.21.self_attn.v_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.21.self_attn.o_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.21.mlp.gate_proj.weight--------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.21.mlp.up_proj.weight----------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.21.mlp.down_proj.weight--------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[4096, 11008]---torch.float16  
model.layers.21.input_layernorm.weight------------------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.21.post_attention_layernorm.weight---------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.22.self_attn.q_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.22.self_attn.k_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.22.self_attn.v_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.22.self_attn.o_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.22.mlp.gate_proj.weight--------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.22.mlp.up_proj.weight----------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.22.mlp.down_proj.weight--------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[4096, 11008]---torch.float16  
model.layers.22.input_layernorm.weight------------------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.22.post_attention_layernorm.weight---------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.23.self_attn.q_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.23.self_attn.k_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.23.self_attn.v_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.23.self_attn.o_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.23.mlp.gate_proj.weight--------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.23.mlp.up_proj.weight----------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.23.mlp.down_proj.weight--------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[4096, 11008]---torch.float16  
model.layers.23.input_layernorm.weight------------------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.23.post_attention_layernorm.weight---------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.24.self_attn.q_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.24.self_attn.k_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.24.self_attn.v_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.24.self_attn.o_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.24.mlp.gate_proj.weight--------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.24.mlp.up_proj.weight----------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.24.mlp.down_proj.weight--------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[4096, 11008]---torch.float16  
model.layers.24.input_layernorm.weight------------------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.24.post_attention_layernorm.weight---------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.25.self_attn.q_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.25.self_attn.k_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.25.self_attn.v_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.25.self_attn.o_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.25.mlp.gate_proj.weight--------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.25.mlp.up_proj.weight----------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.25.mlp.down_proj.weight--------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[4096, 11008]---torch.float16  
model.layers.25.input_layernorm.weight------------------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.25.post_attention_layernorm.weight---------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.26.self_attn.q_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.26.self_attn.k_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.26.self_attn.v_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.26.self_attn.o_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.26.mlp.gate_proj.weight--------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.26.mlp.up_proj.weight----------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.26.mlp.down_proj.weight--------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[4096, 11008]---torch.float16  
model.layers.26.input_layernorm.weight------------------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.26.post_attention_layernorm.weight---------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.27.self_attn.q_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.27.self_attn.k_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.27.self_attn.v_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.27.self_attn.o_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.27.mlp.gate_proj.weight--------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.27.mlp.up_proj.weight----------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.27.mlp.down_proj.weight--------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[4096, 11008]---torch.float16  
model.layers.27.input_layernorm.weight------------------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.27.post_attention_layernorm.weight---------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.28.self_attn.q_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.28.self_attn.k_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.28.self_attn.v_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.28.self_attn.o_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.28.mlp.gate_proj.weight--------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.28.mlp.up_proj.weight----------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.28.mlp.down_proj.weight--------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[4096, 11008]---torch.float16  
model.layers.28.input_layernorm.weight------------------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.28.post_attention_layernorm.weight---------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.29.self_attn.q_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.29.self_attn.k_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.29.self_attn.v_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.29.self_attn.o_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.29.mlp.gate_proj.weight--------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.29.mlp.up_proj.weight----------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.29.mlp.down_proj.weight--------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[4096, 11008]---torch.float16  
model.layers.29.input_layernorm.weight------------------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.29.post_attention_layernorm.weight---------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.30.self_attn.q_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.30.self_attn.k_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.30.self_attn.v_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.30.self_attn.o_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.30.mlp.gate_proj.weight--------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.30.mlp.up_proj.weight----------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.30.mlp.down_proj.weight--------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[4096, 11008]---torch.float16  
model.layers.30.input_layernorm.weight------------------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.30.post_attention_layernorm.weight---------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.31.self_attn.q_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.31.self_attn.k_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.31.self_attn.v_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.31.self_attn.o_proj.weight-----------------------------torch.nn.modules.linear.Linear------------------------------16777216-----------[4096, 4096]---torch.float16  
model.layers.31.mlp.gate_proj.weight--------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.31.mlp.up_proj.weight----------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[11008, 4096]---torch.float16  
model.layers.31.mlp.down_proj.weight--------------------------------torch.nn.modules.linear.Linear------------------------------45088768----------[4096, 11008]---torch.float16  
model.layers.31.input_layernorm.weight------------------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.layers.31.post_attention_layernorm.weight---------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
model.norm.weight---------------------------------------------------transformers.models.llama.modeling_llama.LlamaRMSNorm--------------4096-----------------[4096]---torch.float16  
lm_head.weight------------------------------------------------------torch.nn.modules.linear.Linear-----------------------------131072000----------[32000, 4096]---torch.float16  
Total num: 6738415616 is 6.738415616 Billion. GPU mem: 12.582534790039062 GB
End. After sparse 


Eval after sparse:
The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. 
The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. 
The class this function is called from is 'LlamaTokenizer'.
Dataset: wikitext2
Evaluating ...
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
Perplexity: 7.218737
The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. 
The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. 
The class this function is called from is 'LlamaTokenizer'.
Dataset: ptb
Evaluating ...
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
Perplexity: 12.772421
The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. 
The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. 
The class this function is called from is 'LlamaTokenizer'.
Repo card metadata block was not found. Setting CardData to empty.
Repo card metadata block was not found. Setting CardData to empty.
Dataset: c4
Evaluating ...
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
Perplexity: 9.306179
(torch2) [zk@localhost sparsegpt]$ 