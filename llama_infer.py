import transformers
import torch


from transformers import AutoTokenizer, AutoModelForCausalLM

from transformers import LlamaTokenizer
tokenizer = LlamaTokenizer.from_pretrained("decapoda-research/llama-7b-hf")


model_path = "decapoda-research/llama-7b-hf"
# model_path = "models/llama-prune-0.5"
model = AutoModelForCausalLM.from_pretrained("decapoda-research/llama-7b-hf",device_map="auto")

pipeline = transformers.pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    torch_dtype=torch.bfloat16,
    trust_remote_code=True,
    device_map="auto",
)

def func():
    sequences = pipeline(
    """Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.
    Daniel: Hello, Girafatron!
    Girafatron:""",
        max_length=200,
        do_sample=True,
        top_k=10,
        num_return_sequences=1,
        eos_token_id=tokenizer.eos_token_id,
    )

    for seq in sequences:
        print(f"Result: {seq['generated_text']}")


for i in range(10):
    func()


ts = []
import time
for i in range(100):
    print("="*100)
    print(i)
    start = time.time()
    func()
    ts.append(time.time()-start)
    print(f"{i}: {ts[-1]}")
    
print(model_path, sum(ts)/len(ts), ts)

# 推理都是27G显存
# decapoda-research/llama-7b-hf 3.9504673886299133 [3.5647740364074707, 3.6571288108825684, 3.6103427410125732, 3.665837049484253, 3.6224873065948486, 3.641115665435791, 3.932966709136963, 4.035323858261108, 4.068281888961792, 3.82181715965271, 4.058596849441528, 4.042834043502808, 4.312787055969238, 3.8154351711273193, 3.82196307182312, 3.826543092727661, 4.214095592498779, 4.049176454544067, 4.03291130065918, 4.0379180908203125, 4.002790689468384, 4.32575535774231, 3.8858864307403564, 3.796992540359497, 3.791626453399658, 3.789691686630249, 4.205435037612915, 3.9874134063720703, 4.008742332458496, 3.959986448287964, 3.815415859222412, 4.013921022415161, 4.2776689529418945, 3.7958884239196777, 3.793828248977661, 3.7941927909851074, 4.18700909614563, 3.797880172729492, 4.008472204208374, 4.029498815536499, 4.074145793914795, 4.07178258895874, 4.382213830947876, 3.902797222137451, 3.830292224884033, 3.830502986907959, 4.193057537078857, 3.996774911880493, 4.007720708847046, 3.787120819091797, 3.988720178604126, 4.013220310211182, 4.2956836223602295, 3.856250286102295, 3.786972761154175, 3.792644739151001, 4.00222110748291, 3.983208417892456, 3.9983134269714355, 4.004958868026733, 4.008577585220337, 4.015275001525879, 3.851865291595459, 4.207736253738403, 3.785072088241577, 3.787545919418335, 3.7830820083618164, 4.212610244750977, 4.010077238082886, 3.7924046516418457, 3.9951224327087402, 4.000950813293457, 4.0175251960754395, 4.2892725467681885, 3.78652024269104, 3.7901525497436523, 3.7945539951324463, 3.793410301208496, 4.165369987487793, 4.010621547698975, 4.011205434799194, 4.017019748687744, 3.890108108520508, 3.891697645187378, 4.2869651317596436, 3.788949489593506, 3.7894375324249268, 3.7981631755828857, 4.184300661087036, 3.783513069152832, 4.0012617111206055, 3.9858627319335938, 3.99631929397583, 3.989475727081299, 4.298036813735962, 3.7775371074676514, 3.779000759124756, 3.8006691932678223, 3.783769130706787, 4.196666240692139]
# 使用剪枝0.5+8bit量化
# models/llama-prune-0.5 4.142012615203857 [3.824275016784668, 3.823160409927368, 3.877967596054077, 3.8831119537353516, 4.166847467422485, 5.2497570514678955, 3.8741283416748047, 3.8686814308166504, 3.8998703956604004, 3.8647563457489014, 5.5674729347229, 4.030494689941406, 3.916461706161499, 3.871774673461914, 3.8322839736938477, 3.8344333171844482, 7.919975519180298, 5.10054874420166, 3.8372015953063965, 3.837045431137085, 3.823533058166504, 3.825507164001465, 3.8276193141937256, 3.8221681118011475, 3.8166258335113525, 3.8261618614196777, 3.820256233215332, 3.8250386714935303, 3.825737953186035, 3.817899703979492, 3.822436809539795, 3.8531923294067383, 6.452220439910889, 3.826960325241089, 3.8450117111206055, 3.8407511711120605, 3.8497743606567383, 3.832571268081665, 5.427156448364258, 3.824667453765869, 3.8269224166870117, 3.8234288692474365, 3.8132572174072266, 4.056239604949951, 5.181805849075317, 3.813553810119629, 3.8177757263183594, 3.839082717895508, 3.828707218170166, 4.690397500991821, 4.560275316238403, 3.8360493183135986, 3.8313426971435547, 3.8332433700561523, 3.8300981521606445, 5.414725303649902, 3.8419618606567383, 3.8391456604003906, 3.8360326290130615, 3.8335444927215576, 3.82863450050354, 5.429534912109375, 3.8563148975372314, 3.843700885772705, 3.8317906856536865, 3.8416969776153564, 3.8399322032928467, 7.514410018920898, 5.339208602905273, 3.865955114364624, 3.8757083415985107, 3.8612070083618164, 3.8531553745269775, 3.870781183242798, 3.829967737197876, 3.826730728149414, 3.816349506378174, 3.8135385513305664, 3.8208789825439453, 3.826979875564575, 3.8229691982269287, 3.8347065448760986, 5.564488172531128, 4.711624383926392, 3.8285751342773438, 3.8385021686553955, 3.8318207263946533, 3.8239917755126953, 5.417190074920654, 3.8259193897247314, 3.8191721439361572, 3.825589418411255, 3.832319498062134, 3.817328453063965, 5.418643951416016, 3.8666887283325195, 3.8341360092163086, 3.820157766342163, 3.821619749069214, 3.8262135982513428]

